<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Zhou&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Zhou&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Zhou&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Zhou's blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zhou&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-NeuralNetwork" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/11/NeuralNetwork/" class="article-date">
  <time class="dt-published" datetime="2022-04-11T13:54:13.000Z" itemprop="datePublished">2022-04-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/04/11/NeuralNetwork/">NeuralNetwork</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Foundmental"><a href="#Foundmental" class="headerlink" title="Foundmental"></a>Foundmental</h2><img src="/2022/04/11/NeuralNetwork/SimpleNetwork.png" class title="This is an example image">
<blockquote>
<p>简单神经网络,output&#x3D;sigma((w1<em>x1+w2</em>x2+…wn*xn)+b)</p>
</blockquote>
<hr>
<h3 id="Activate-Function"><a href="#Activate-Function" class="headerlink" title="Activate Function"></a>Activate Function</h3><img src="/2022/04/11/NeuralNetwork/sigmod.jpg" class title="This is an example image">  
<blockquote>
<p>它对每个神经元的输出进行了归一化，并且取值是0到1，因此非常适合作为概率预测（逻辑回归问题），但是它存在梯度消失现象。</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/tanh.jpg" class title="This is an example image"> 
<blockquote>
<p>它与sigmoid相似，但是tanh是以0为中心，并且当值过大或者过小时，它的曲线更加平滑（一般来说tanh更多用于隐藏层中，sigmoid更多应用于输出层）</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/relu.jpg" class title="This is an example image">
<blockquote>
<p>当输入为正的时候，不它不会存在梯度饱和现象并且因为线性关系的原因，计算速度也比sigmoid和tanh更快，不过它存在当输入为负的时候，relu就完全失效了。</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/leakrelu.jpg" class title="This is an example image"> 
<img src="/2022/04/11/NeuralNetwork/Leakrelu1.jpg" class title="This is an example image"> 
<blockquote>
<p>LeakRelu理论上来说具有了Relu的所有有点，通常可以对a的值取一个比较小的值，来调整负值的零梯度问题，这样扩展了传统Relu的范围，应用上尚未证明LeakRelu优于Relu</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/softmax.jpg" class title="This is an example image">
<img src="/2022/04/11/NeuralNetwork/softmax1.jpg" class title="This is an example image">
<blockquote>
<p>SoftMax适用于多分类问题的激活函数，在最后一层对值进行归一化</p>
</blockquote>
<hr>
<h3 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h3><blockquote>
<p>Grandient descent也可以理解为寻找最小值的方法，在深度学习中，梯度下降就是寻找损失函数最小的时候网络权重的方法。</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/grandientdescent.jpg" class title="This is an example image">

<h4 id="SGD-and-BGD"><a href="#SGD-and-BGD" class="headerlink" title="SGD and BGD"></a>SGD and BGD</h4><blockquote>
<p>假设我们现在有一批样本呢，SGD(随机梯度下降）是每次选择样本中的一个进行迭代，直至样本集中所有样本迭代完成。这样做好处就是可以占用更少的资源，更新速度更快，坏处错误的样本会一定程度影响下降的速度，但是总体还是会朝着最优点下降。相反，BGD(批量梯度下降)是对于这批样本，每次将所有样本放入进行更新，每更新一个样本都需要对所有样本进行一个加和。</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/bgd.jpg" class title="This is an example image">

<h4 id="The-combine-of-them-（Mini-Batch-gradient-descent-）"><a href="#The-combine-of-them-（Mini-Batch-gradient-descent-）" class="headerlink" title="The combine of them （Mini-Batch gradient descent ）"></a>The combine of them （Mini-Batch gradient descent ）</h4><blockquote>
<p>完整的样本集拥有N个样本，每次取b个样本，并且更新这b个样本，SGD是b&#x3D;1的情况，BGD是b&#x3D;N的情况，Mini-batch可以看作两者的折中</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/MBGD.jpg" class title="This is an example image">

<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><blockquote>
<p>由于SGD中 对于某些参数，固定的a（learning rate）学习效率并不高，所以对于学习率采用以下的改进,s一般初始化为0，然后随着参数更新，每次将参数的梯度平方求和累加到s上，伊普西隆通常取一个很小值，它的存在是为了防止s初始化为0的时候，分数趋近于无穷大。η是初始化的learning rate。可以从这个式子中看到，当一个参数的梯度较大的时候，分母就会特别大，这就会导致学习率减小，防止振荡现象产生。如果参数梯度很小，那么相应的让学习率上升，加快更新。</p>
</blockquote>
<img src="/2022/04/11/NeuralNetwork/adagrad.jpg" class title="This is an example image">
<hr>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><blockquote>
<p>总体来说，损失函数是对于你的算法&#x2F;模型在你数据集上表现好坏的评估。它有助于我们了解predict和groundtruth之间的差异。损失函数总体分为三大类，回归，二分类，多分类。<br>常用损失函数：</p>
</blockquote>
<h5 id="Mean-Squared-Error"><a href="#Mean-Squared-Error" class="headerlink" title="Mean Squared Error"></a>Mean Squared Error</h5><img src="/2022/04/11/NeuralNetwork/MSE.jpg" class title="This is an example image">
<h5 id="Mean-Absolute-Erroe"><a href="#Mean-Absolute-Erroe" class="headerlink" title="Mean Absolute Erroe"></a>Mean Absolute Erroe</h5><img src="/2022/04/11/NeuralNetwork/MAE.jpg" class title="This is an example image">
<h5 id="Root-Mean-Squared-Error"><a href="#Root-Mean-Squared-Error" class="headerlink" title="Root Mean Squared Error"></a>Root Mean Squared Error</h5><img src="/2022/04/11/NeuralNetwork/RMSE.jpg" class title="This is an example image">
<h5 id="Categorical-Cross-Entropy-Cost-Function-分类交叉熵"><a href="#Categorical-Cross-Entropy-Cost-Function-分类交叉熵" class="headerlink" title="Categorical Cross Entropy Cost Function(分类交叉熵)"></a>Categorical Cross Entropy Cost Function(分类交叉熵)</h5><img src="/2022/04/11/NeuralNetwork/CCECF.jpg" class title="This is an example image">
<h5 id="Binary-Cross-Entropy-Cost-Function"><a href="#Binary-Cross-Entropy-Cost-Function" class="headerlink" title="Binary Cross Entropy Cost Function"></a>Binary Cross Entropy Cost Function</h5><img src="/2022/04/11/NeuralNetwork/BCE.jpg" class title="This is an example image">
<h5 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL-Divergence"></a>KL-Divergence</h5><img src="/2022/04/11/NeuralNetwork/KL.jpg" class title="This is an example image">
<blockquote>
<p>KL散度是一个分布与另一个分布的概率差异的度量，KL散度在功能上类似于多类交叉熵，KL散度不能用于距离函数，因为它不是对称的。</p>
</blockquote>
<h5 id="Pairwise-Ranking-Loss"><a href="#Pairwise-Ranking-Loss" class="headerlink" title="Pairwise Ranking Loss"></a>Pairwise Ranking Loss</h5><img src="/2022/04/11/NeuralNetwork/PRL.jpg" class title="This is an example image">
<blockquote>
<p>我们从上式可以看到，当两个人的描述的是一个人时，他们嵌入表示距离大小就是loss，当描述不是一个人时，嵌入表示距离大于margin才不会产生loss。</p>
</blockquote>
<h5 id="Triplet-Ranking-Loss"><a href="#Triplet-Ranking-Loss" class="headerlink" title="Triplet Ranking Loss"></a>Triplet Ranking Loss</h5><img src="/2022/04/11/NeuralNetwork/TRL.jpg" class title="This is an example image">
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/11/NeuralNetwork/" data-id="cl1utw3dz0000b0uegxck7wgp" data-title="NeuralNetwork" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/11/NeuralNetwork/">NeuralNetwork</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Zhou<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>